# ML Model Evaluation and Hyperparameter Tuning

This project demonstrates how to evaluate multiple classification models and optimize their performance using hyperparameter tuning techniques like **GridSearchCV** and **RandomizedSearchCV**.

## 📌 Dataset
Used the **Breast Cancer** dataset from `scikit-learn`, which is a binary classification problem (malignant vs benign tumors).

## 🔧 Models Used
- Logistic Regression
- Support Vector Machine (SVM)
- Random Forest

## 📊 Evaluation Metrics
- Accuracy
- Precision
- Recall
- F1 Score
- Classification Report

## 🔍 Hyperparameter Tuning
Performed tuning on the Random Forest classifier using:
- **GridSearchCV**: Exhaustive search over parameter grid
- **RandomizedSearchCV**: Random sampling of parameters

## ✅ Results
Both tuned models achieved the following:
- **Accuracy**: 96.49%
- **Precision**: 95.89%
- **Recall**: 98.59%
- **F1-Score**: 97.22%

## 🧪 Tools & Libraries
- Python
- scikit-learn
- pandas
- numpy
- matplotlib
- seaborn
- Jupyter Notebook

## 📁 Project Structure

