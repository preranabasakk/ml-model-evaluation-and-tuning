# ML Model Evaluation and Hyperparameter Tuning

This project demonstrates how to evaluate multiple classification models and optimize their performance using hyperparameter tuning techniques like **GridSearchCV** and **RandomizedSearchCV**.

## ğŸ“Œ Dataset
Used the **Breast Cancer** dataset from `scikit-learn`, which is a binary classification problem (malignant vs benign tumors).

## ğŸ”§ Models Used
- Logistic Regression
- Support Vector Machine (SVM)
- Random Forest

## ğŸ“Š Evaluation Metrics
- Accuracy
- Precision
- Recall
- F1 Score
- Classification Report

## ğŸ” Hyperparameter Tuning
Performed tuning on the Random Forest classifier using:
- **GridSearchCV**: Exhaustive search over parameter grid
- **RandomizedSearchCV**: Random sampling of parameters

## âœ… Results
Both tuned models achieved the following:
- **Accuracy**: 96.49%
- **Precision**: 95.89%
- **Recall**: 98.59%
- **F1-Score**: 97.22%

## ğŸ§ª Tools & Libraries
- Python
- scikit-learn
- pandas
- numpy
- matplotlib
- seaborn
- Jupyter Notebook

## ğŸ“ Project Structure

